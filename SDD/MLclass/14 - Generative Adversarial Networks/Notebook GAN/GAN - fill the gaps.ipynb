{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "\n",
    "**Author: Thomas Pierrot**\n",
    "( inspired from a notebook from Batipste Jorant and Thomas Pierrot )\n",
    "\n",
    "*A short tutorial to explain and experiment how GANs work through easy examples. We will be using Pytorch during this notebook.*\n",
    "\n",
    "##### First let's import some prerequisites, while you'll read the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Motivation\n",
    "\n",
    ">**Yann LeCun**, *Director of AI Research at Facebook and Professor at NYU* :\n",
    ">\n",
    ">\n",
    ">>\"There are many interesting recent development in deep learning, probably too many for me to describre them all here. But there are a few ideas that caught my attention enough for me to get personally involved in research projects.\n",
    ">>\n",
    ">>The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks). This is an idea that was originally proposed by Ian Goodfellow whe he was a student with Yoshua Bengio at the University of Montreal (he since moved to Google Brain and recently to OpenAI).\n",
    ">>\n",
    ">>This, and the variations that are now being proposed is the most interesting idea in the last  years in ML, in my opinion.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the initial goal of GANs ? \n",
    "\n",
    "<div class=\"alert alert-success\" style=\"margin-top: 1em\">\n",
    "<b>Generative Adversarial Networks (GAN)</b> are a new framework for generating models. <b>The idea is to generate data that looks real, but that does not already exist.</b><br/>\n",
    "For example, generating images of hand-written numbers that seem man made by training such a network on the MNIST database. \n",
    "</div> \n",
    "\n",
    "In this framework, two assumptions have to be made:\n",
    "<ul>\n",
    "<li> All the data from the training dataset is assumed to follow the same probability distribution noted $p_{data}$. In other words, if the MNIST database is considered, all the 28x28 matrices which represent hand-written numbers are assumed to follow the same probability distribution.</li>\n",
    "    \n",
    "\n",
    "<li> The database is assumed large enough that the probability distribution obtained represent well and only those data. In other words, if new sample can be generated from this distribution, they should look like the other elements of the database. In the case of the MNIST database, if new 28x28 matrices may be generated from the distribution $p_{data}$, then they should still represent hand-written numbers.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "As suggested above, we will practise on Mnist database. Basically, we will learn to our computer how to write figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch_size\n",
    "batch_size = 100\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "# we normalize data to have values between -1 and 1\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]) ])\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "train_iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some examples of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16 # number of digits to plot\n",
    "\n",
    "# create figure for plotting\n",
    "size_figure_grid = int(math.sqrt(num_test_samples))\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "\n",
    "# load a batch of training data\n",
    "images, labels = next(train_iterator)\n",
    "\n",
    "# show a subpart of it\n",
    "for k in range(num_test_samples):\n",
    "    i = k//4\n",
    "    j = k%4\n",
    "    ax[i,j].cla()\n",
    "    ax[i,j].imshow(images[k,:].data.cpu().numpy().reshape(28, 28), cmap='Greys')\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train such a network ?\n",
    "\n",
    "##### The Generator\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "A first network - called <b>Generator</b>- is built and is meant to transform vectors following a random probability distribution $p_z$ - called noise probability distribution - in vectors following natural data distribution $p_{data}$.<br/>\n",
    "<br/>\n",
    "<b>In other words,</b> to create fake data from scratch that looks like the real data from the DB.\n",
    "\n",
    "</div>\n",
    "\n",
    "To keep the MNIST example, a generator could take as input vectors of real numbers following a gaussian distribution and should output a 28x28 matrix whose each value follows the data probability distribution followed by MNIST images. Hence, that network role would be to transform a gaussian distribution into a probability distribution followed by natural - and in this case man made - samples.\n",
    "\n",
    "##### The Discriminator\n",
    "\n",
    "To train the generator, Ian Goodfellow suggested to build a second network called the **Discriminator**. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The <b>Discriminator</b>'s goal is to make the difference between samples following the real data probability distribution and samples generated by the generator. It takes as an input either a real sample or a sample output from the generator - called fake sample - and is trained to return the likelyhood of the image being a real one. <br/>\n",
    "<br/>\n",
    "<b>In other words,</b> the discriminator is expected to return 1 when it is fed with an image from the training dataset and to return 0 when it is fed with an image generated by the generator.\n",
    "</div>\n",
    "\n",
    "Then the generator is trained to fool the discriminator : ie to make it return 1 for fake samples too. Therefore, both network play together a 2 players minimax game. At the equilibrium, the discriminator should always return 0.5 : it cannot make the difference between real and fake samples anymore, hence the fake samples follow the same probability distribution than real ones. The generator is trained that way and produces real-like results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more mathematical point of view\n",
    "To get a better understanding, let us call G the generator, D the discriminator, $z$ a variable following the noise probability distribution $p_z$ and $x$ a variable following the data probability distribution $p_{data}$. Let us call as well $p_g$ the probability distribution followed by Gâ€™s outputs : $G(z)$. Then the generator goal is to fool the discriminator returning real like samples which means equalizing $p_g$ and $p_{data}$ . Hence, the discriminator is trained to return $1$ for real data : $D(x) = 1$ and $0$ for fake data $D(G(z)) = 0$ while the generator is trained to make the discriminator returning $1$ as well for fake data. Hence, D and G play the two-player minimax game with value function $V(G, D)$ suggested by Ian Goodfellow:\n",
    "\n",
    "$$\\min_{G} \\max_{D} V(D,G)  = \\mathbb{E}_{x\\sim p_{data}}[log D(x)] + \\mathbb{E}_{z\\sim p_{z}}[log(1 - D(G(z)))]$$\n",
    "\n",
    "The implementation of this game results in the following algorithm, described in the original GAN paper:\n",
    "\n",
    "<img src=\"./Images/gan_algo.png\" style=\"max-width:100%; width: 70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to implement our first GAN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural networks\n",
    "\n",
    "We now build both the networks. In this notebook, as in the original paper, both will be simple fully connected network.\n",
    "\n",
    "<img src=\"./Images/schema_gan_mnist.png\" style=\"max-width:100%; width: 70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's your turn!**\n",
    "\n",
    "We give you the **Discriminator** code, you can take inspiration from it for the **Generator**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x.view(x.size(0), 784))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # ToDo: write the generator code (it takes a noise of size 100 as input)\n",
    "    # Layers : - Linear 256 neurons, activation: LeakyReLu (negative slope: 0.2, inplace: True)\n",
    "    #          - Linear 512 neurons, activation: LeakyReLu (negative slope: 0.2, inplace: True)\n",
    "    #          - Linear 1024 neurons, activation: LeakyReLu (negative slope: 0.2, inplace: True)\n",
    "    #          - Linear ? neurons, activation: ?\n",
    "    # I let you guess the last layer number of neurons and activation\n",
    "    # Tip: this network output flat vectors (they will be reshaped as square images later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the class are written, we instantiate the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialize the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() #BCE standing for Binary Cross Entropy\n",
    "lr = 0.0002\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write both the discriminator and generator train functions.\n",
    "\n",
    "Tip : criterion(batch_a, batch_b) returns esp_{batch} log(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, images, real_labels, fake_images, fake_labels):\n",
    "    discriminator.zero_grad()\n",
    "    # ToDo: complete the code\n",
    "    # images is a batch of data from the dataset\n",
    "    # fake-images is a batch of images generated by the generator\n",
    "    # real_labels is a vector full of 1\n",
    "    # fake_labels is a vector full of 0\n",
    "\n",
    "    d_loss = #?\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss, real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator_outputs, real_labels):\n",
    "    generator.zero_grad()\n",
    "    # ToDo complete the code\n",
    "    \n",
    "    g_loss = # ?\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model!\n",
    "The moment of truth : we will train GAN networks on our database. \n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Warning :</b>\n",
    "This operation will take some time, depending on the value of the batch and the number of epochs you defined, and on your computer performances.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare folder to store results\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "    \n",
    "# Prepare folder to store models\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples from the input distribution to inspect the generation on training \n",
    "num_test_samples = 16\n",
    "test_noise = Variable(torch.randn(num_test_samples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure for plotting\n",
    "size_figure_grid = int(math.sqrt(num_test_samples))\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "\n",
    "# Set number of epochs and initialize figure counter\n",
    "num_epochs = 25\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "# Set counter\n",
    "num_fig = 0\n",
    "\n",
    "# Start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):  \n",
    "    for n, (images, _) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert data to suitable format\n",
    "        images = Variable(images)\n",
    "        real_labels = Variable(torch.ones(images.size(0), 1))\n",
    "        \n",
    "        # Sample from generator\n",
    "        noise = Variable(torch.randn(images.size(0), 100))\n",
    "        fake_images = generator(noise)\n",
    "        fake_labels = Variable(torch.zeros(images.size(0), 1))\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss, real_score, fake_score = train_discriminator(discriminator, images, real_labels, fake_images, fake_labels)\n",
    "        \n",
    "        # Sample again from the generator and get output from discriminator\n",
    "        noise = Variable(torch.randn(images.size(0), 100))\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = train_generator(generator, outputs, real_labels)\n",
    "        \n",
    "        # Every half epoch generates pictures with to generator to monitor training\n",
    "        if (n+1) % int(num_batches/2) == 0:\n",
    "            # generate pictures\n",
    "            test_images = generator(test_noise)\n",
    "            \n",
    "            # plot them\n",
    "            for k in range(num_test_samples):\n",
    "                i = k//4\n",
    "                j = k%4\n",
    "                ax[i,j].cla()\n",
    "                ax[i,j].imshow(test_images[k,:].data.cpu().numpy().reshape(28, 28), cmap='Greys')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "            # save the picture\n",
    "            plt.savefig('results/mnist-gan-%03d.png'%num_fig)\n",
    "            num_fig += 1\n",
    "            # print log\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "                  'D(x): %.2f, D(G(z)): %.2f, time %.2f min' \n",
    "                  %(epoch + 1, num_epochs, n+1, num_batches, d_loss.detach().numpy(), g_loss.detach().numpy(),\n",
    "                    real_score.detach().numpy().mean(), fake_score.detach().numpy().mean(), (time.time()-t0)/60))\n",
    "            \n",
    "        # at the end of each epoch, save the models\n",
    "        torch.save(generator.state_dict(), os.path.join('models', 'generator.pkl'))\n",
    "        torch.save(discriminator.state_dict(), os.path.join('models', 'discriminator.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And an hand-made comics to end this notebook, enjoy :)\n",
    "Thank you baptiste !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/Melusine1.jpg\" style=\"width: 700px;\">\n",
    "\n",
    "<img src=\"./Images/Melusine2.jpg\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
