{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text classification in English\n",
    "\n",
    "## 1.1 Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part1\\3-378msg5.txt\n",
      "email is a spam: False\n",
      "Subject: t\n",
      "\n",
      "hi , help ! i have to design an experiment to do with mandarin tones as part of a phonology requirement on my graduate course . there seems to be very little literature on this in the library . if anyone can think of any on-going debates on the phonology / phonetics of mandarin tones for which an experiment would be useful , please could you give me information and references . i would welcome any suggestions at all . thanks a lot , sophia wang . ( sophia @ ling . ed . ac . uk )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DURANTGA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DURANTGA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LemmaTokenizer defined above will be applied further in this example. The next step is to define the Count Vectorization pipeline using this Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "bow = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Using the bag of words (BOW) object to classify spam\n",
    "\n",
    "Let's start by splitting the data into train and test sets, using 20% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow,email_label,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example we will use a Logistic Regression Classifier. Let's fit it to our Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9930915371329879\n",
      "Precision : 0.9891304347826086\n",
      "Recall : 0.9680851063829787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr_classifier=LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, Bag of Words can provide sufficient information for classification. In this case, the accuracy reached by our classifier is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: re : 2 . 882 s - > np np\n",
      "\n",
      "> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that ' john mcnamara the name ' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were ' chaim shmendrik ' , ' john mcnamara the name ' would be false . no tautology , this . ( and no reduplication , either . )\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "52566",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-283b826d509e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0memailBagOfWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfeat2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmail_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmail_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bag of words representation (\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailBagOfWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" words in dict):\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailBagOfWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-283b826d509e>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0memailBagOfWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mfeat2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmail_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmail_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bag of words representation (\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailBagOfWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" words in dict):\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailBagOfWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 52566"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(bow)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the classification process again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9706390328151986\n",
      "Precision : 1.0\n",
      "Recall : 0.83\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf,email_label,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simplae case, additional filtering is unecessary and even removed some information. There is indeed likely a link between the abundance of words/long emails and the fact that this email is a spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text classification in French\n",
    "\n",
    "The previously used dataset is a widely used dataset for introductory text classification. \n",
    "\n",
    "The field of Natural Language Understanding, and Natural Language Classification in particular, suffers from two challenges :\n",
    "- Adapting the features and methodologies to various and more complex datasets\n",
    "- Adapting the process to languages other than english\n",
    "\n",
    "Concerning the latter, one has to take into account that most of NLU research is currently performed on english. Datasets are rarely available for other languages, and the algorithms proposed for better NLU are often left untested on foreign data. \n",
    "French, for instance, has less efficient lemmatization (french is a richly flected language). In the following section, we will reuse the same methodologies on a french dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>game_fr</th>\n",
       "      <th>game_en</th>\n",
       "      <th>platform</th>\n",
       "      <th>website_rating</th>\n",
       "      <th>public_rating</th>\n",
       "      <th>publishor/developer</th>\n",
       "      <th>release</th>\n",
       "      <th>type</th>\n",
       "      <th>classification</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.hack//G.U. Last Recode</td>\n",
       "      <td>.hack//G.U. Last Recode</td>\n",
       "      <td>PS4</td>\n",
       "      <td>14/20</td>\n",
       "      <td>--/20</td>\n",
       "      <td>Bandai Namco Entertainment</td>\n",
       "      <td>03 Novembre 2017</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/ps4/jeu-674262/</td>\n",
       "      <td>Au contraire d’autres titres, None ,''.hack'' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.hack//G.U. Vol.1//Rebirth</td>\n",
       "      <td>.hack//G.U. Vol. 1//Rebirth</td>\n",
       "      <td>PS2</td>\n",
       "      <td>15/20</td>\n",
       "      <td>18.2/20</td>\n",
       "      <td>Bandai Namco CyberConnect2</td>\n",
       "      <td>Date de sortie inconnue</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+7 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Avec plus de 20 œuvres de fiction sur de mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.hack//Infection Part 1</td>\n",
       "      <td>.hack//Infection: Part 1</td>\n",
       "      <td>PS2</td>\n",
       "      <td>15/20</td>\n",
       "      <td>15.1/20</td>\n",
       "      <td>CyberConnect2 Bandai</td>\n",
       "      <td>26 Mars 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>S'appuyant sur la maxime « Mieux vaut tard qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.hack//Mutation Part 2</td>\n",
       "      <td>.hack//Mutation: Part 2</td>\n",
       "      <td>PS2</td>\n",
       "      <td>14/20</td>\n",
       "      <td>16.4/20</td>\n",
       "      <td>Bandai CyberConnect2</td>\n",
       "      <td>18 Juin 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Voici enfin le second volet de la quadrilogie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.hack//Outbreak Part 3</td>\n",
       "      <td>.hack//Outbreak: Part 3</td>\n",
       "      <td>PS2</td>\n",
       "      <td>13/20</td>\n",
       "      <td>15.3/20</td>\n",
       "      <td>CyberConnect2 Atari</td>\n",
       "      <td>17 Septembre 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Comme la maxime «Jamais deux sans trois» ne c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     game_fr                      game_en  \\\n",
       "0           1     .hack//G.U. Last Recode      .hack//G.U. Last Recode   \n",
       "1           2  .hack//G.U. Vol.1//Rebirth  .hack//G.U. Vol. 1//Rebirth   \n",
       "2           3     .hack//Infection Part 1     .hack//Infection: Part 1   \n",
       "3           4      .hack//Mutation Part 2      .hack//Mutation: Part 2   \n",
       "4           5      .hack//Outbreak Part 3      .hack//Outbreak: Part 3   \n",
       "\n",
       "  platform website_rating public_rating         publishor/developer  \\\n",
       "0      PS4          14/20         --/20  Bandai Namco Entertainment   \n",
       "1      PS2          15/20       18.2/20  Bandai Namco CyberConnect2   \n",
       "2      PS2          15/20       15.1/20        CyberConnect2 Bandai   \n",
       "3      PS2          14/20       16.4/20        Bandai CyberConnect2   \n",
       "4      PS2          13/20       15.3/20         CyberConnect2 Atari   \n",
       "\n",
       "                   release type classification  \\\n",
       "0         03 Novembre 2017  RPG        +12 ans   \n",
       "1  Date de sortie inconnue  RPG         +7 ans   \n",
       "2             26 Mars 2004  RPG        +12 ans   \n",
       "3             18 Juin 2004  RPG        +12 ans   \n",
       "4        17 Septembre 2004  RPG        +12 ans   \n",
       "\n",
       "                                                 url  \\\n",
       "0      http://www.jeuxvideo.com/jeux/ps4/jeu-674262/   \n",
       "1  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "2  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "3  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "4  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "\n",
       "                                         description  \n",
       "0  Au contraire d’autres titres, None ,''.hack'' ...  \n",
       "1   Avec plus de 20 œuvres de fiction sur de mult...  \n",
       "2   S'appuyant sur la maxime « Mieux vaut tard qu...  \n",
       "3   Voici enfin le second volet de la quadrilogie...  \n",
       "4   Comme la maxime «Jamais deux sans trois» ne c...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load video games reviews\n",
    "vgr = pd.read_csv(\"datasets/jvc.csv\")\n",
    "vgr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17efb1fc978>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert rating to numeric values, and plot the histogram of values\n",
    "rating=vgr.website_rating.apply(lambda k: k[:-3])\n",
    "vgr['rating']=pd.to_numeric(rating)\n",
    "vgr.rating.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most games seem to have a rating between 11 and 16. In this exercise, we will try to determine if we can determine if a game is very good (rating above 16) or very bad (rating below 11) based only on the summary of its review.\n",
    "\n",
    "Let's start by splitting the dataset between good and bad games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "bad=vgr[(vgr.rating<=11) & (vgr.platform==\"PC\")]\n",
    "bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
    "good=vgr[(vgr.rating>=16) & (vgr.platform==\"PC\")]\n",
    "good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n",
    "selected_games=pd.concat([good,bad]).dropna()\n",
    "\n",
    "#Keep only reviews and \n",
    "game_reviews=selected_games['description']\n",
    "game_quality=selected_games['quality']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization in French is a tricky issue.\n",
    "\n",
    "One example : the verb finir can be expressed as finissons, finirez, finisse, finit, etc...\n",
    "Lemmatization is typically less efficient in french than in english. \n",
    "\n",
    "Another alternative is to use Stemming instead. Stemming uses RegEx rules to truncate the end of a word that would normally correspond to conjugations, inflections, etc...\n",
    "Stemming destructs the readability of the words by truncating their end, but runs faster than Lemmatization\n",
    "\n",
    "In the next cell, we adapt the LemmaTokenizer that we defined earlier using a FrenchStemmer instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class FrenchStemTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.st = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.st.stem(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
    "bow_games = countvect.fit_transform(game_reviews)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2349\n",
      "Number of words: 3161\n",
      "Document - words matrix: (2349, 3161)\n",
      "First words: ['abandon', 'abomin', 'abord', 'abras', 'absenc', 'absolut', 'absorb', 'abstract', 'abyssal', 'academy', 'accent', 'accept', 'access', 'accessibl', 'acclaim', 'accord', 'accouch', 'accru', 'accus', 'ace', 'aci', 'acolyt', 'acquisit', 'acquit', 'acquitt', 'act', 'action', 'activ', 'activity', 'actual', 'ad', 'adag', 'adapt', 'add', 'addict', 'addit', 'adieu', 'adieux', 'admir', 'adolescent', 'adopt', 'ador', 'adrenalin', 'adroit', 'advanc', 'advanced', 'adventur', 'advers', 'aero', 'affect', 'affili', 'affirm', 'affluenc', 'afflux', 'affront', 'afraid', 'after', 'afterbirth', 'against', 'age', 'agend', 'agent', 'aggress', 'agit', 'agon', 'agricol', 'agricultur', 'ah', 'aid', 'aiguill', 'aim', 'aion', 'air', 'al', 'alan', 'album', 'alfa', 'ali', 'alien', 'align', 'aliment', 'aliv', 'all', 'allan', 'allemand', 'aller', 'allianc', 'allur', 'allus', 'almost', 'alon', 'alpha', 'alphabet', 'altern', 'am', 'amass', 'amateur', 'amatric', 'ambit', 'ambivalent']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(game_reviews))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow_games.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6808510638297872\n",
      "Precision : 0.7008547008547008\n",
      "Recall : 0.6721311475409836\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2349, 3161)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_games = TfidfTransformer().fit_transform(bow_games)\n",
    "tfidf_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6510638297872341\n",
      "Precision : 0.6382978723404256\n",
      "Recall : 0.743801652892562\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tfidf and Bow are usually very efficient features to manipulate during cassification. However, please note that their size is directly related to the size of the vocabulary of our corpus. \n",
    "\n",
    "In our current example, even when using stemming, the dimensionnality of our bow or tfidf vectors is still very high (3161). This is not maintaintable with increasing corpora sizes.\n",
    "\n",
    "In this sction, we will use the word2vec embeddings, that address this issue by proposing an architecture that learns individual representations for words in a vector space of given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.stem(t) for t in word_list]\n",
    "\n",
    "tok=FrenchTokenizer()\n",
    "\n",
    "text_for_word2vec=[tok(sent) for sent in vgr['description'].dropna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=Word2Vec(text_for_word2vec,size=200,window=5,min_count=1)\n",
    "model.save(\"word2vec.model\")\n",
    "w2v=dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oubli', 0.7170845866203308),\n",
       " ('nécessair', 0.7137531042098999),\n",
       " ('facil', 0.7080598473548889),\n",
       " ('intérêt', 0.7036499977111816),\n",
       " ('toutefois', 0.703342854976654),\n",
       " ('potentiel', 0.701346755027771),\n",
       " ('sensat', 0.6931085586547852),\n",
       " ('besoin', 0.6889208555221558),\n",
       " ('boulot', 0.6879130005836487),\n",
       " ('sérieux', 0.6796602010726929)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self,word2vec,dim):\n",
    "        self.word2vec=word2vec\n",
    "        self.dim=dim\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 <__main__.MeanEmbeddingVectorizer object at 0x0000025193670E10>),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(game_reviews,game_quality,test_size=0.2)\n",
    "\n",
    "pipe=Pipeline([('vectorizer',MeanEmbeddingVectorizer(w2v,200)),('classifier',lr_classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.548936170212766\n",
      "Precision : 0.5566502463054187\n",
      "Recall : 0.875968992248062\n"
     ]
    }
   ],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observe here is that word2vec embeddings perform worse than what we learned from BOW or TFIDF. \n",
    "\n",
    "In our case, the training corpus for the embeddings was not large enough to ensure proper convergence and representation of the words.\n",
    "\n",
    "It is also common that for smaller corpora (<10.000 docs approximately), TFIDF usually performs better for classification, whereas Word2Vec produces better results with larger corpora and across domains (e.g. training on data from Wikipedia, and then using the vectors on data from another field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
