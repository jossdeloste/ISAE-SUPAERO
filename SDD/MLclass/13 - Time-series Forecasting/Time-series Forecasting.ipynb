{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-series Forecasting as an introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we'll cover time-series forecasting. This family of algorithms can be used for a variety of prediction tasks, such as [predicting stock prices](https://arxiv.org/pdf/1911.13288.pdf) and [seizure prediction](https://www.sciencedirect.com/science/article/pii/S001048251830132X). We'll focus on the use of Recurrent Neural Networks in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use today is a set of EEG readings used to study seizures. You can find the data [here](https://physionet.org/content/chbmit/1.0.0/), and in the `data` directory there is a single patient recording `chb01_chb01_03.edf` which includes a seizure episode. We won't focus on seizure detection today but will instead try to predict neural activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are stored in the European Data Format, https://www.edfplus.info/. We'll use the python library `pyedflib` to read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyedflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join('data','chb01_chb01_03.edf')\n",
    "f = pyedflib.EdfReader(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = f.signals_in_file\n",
    "labels = f.getSignalLabels()\n",
    "print('%d different signals: %s' % (n, str(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, sharex=True, figsize=(18,5))\n",
    "for i in range(5):\n",
    "    signal = f.readSignal(i)\n",
    "    axs[i].plot(signal[::256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this single recording contains 23 different signals from different sensors placed. We'll focus at predicting a single signal. Our first data processing step is normalizing the data. This is beneficial for recurrent neural network training, but does require domain knowledge. In this case, we know the physical limits of the sensors, so we can use that to normalize the data between $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = f.readSignal(0)\n",
    "signal = 2 * (signal - f.physical_min(0)) / (f.physical_max(0) - f.physical_min(0)) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preparation often done with timeseries data is to check if it is **stationary**, ie if the mean and variance change over time. To do this, we'll use the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test). This test can be very costly to compute, so we'll downsample the data. There are 256 samples per second in this data, so we'll take one sample each second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "p_value = sm.tsa.stattools.adfuller(signal[::256])[1]\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small value means that the timeseries **is stationary**. This isn't a necessary condition for LSTMs, but it will help training. When the timeseries is not stationary (`p_value > 0.05`), it's normal to instead predict the **difference** between timesteps, ie $$y_t - y_{t-1}$$ which can be calculated in numpy using `diff` and in pandas with `shift`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data into training and test sets, perserving order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(9 * len(signal) / 10)\n",
    "train_x = list(range(train_n))\n",
    "train_signal = signal[:train_n]\n",
    "test_x = list(range(train_n, len(signal)))\n",
    "test_signal = signal[train_n:]\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(train_x, train_signal, label='train')\n",
    "plt.plot(test_x, test_signal, label='test')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples we'll be looking at today are\n",
    "+ Univariate: we're using one signal to predict the future of that signal without considering other features. The opposite of this is multivariate, where multiple features are used in prediction (and multiple features can be simultaneously predicted)\n",
    "+ 1-Step: we're predicting one step into the future, which can also be said as having a horizon of one sample. N-step or multi-step timeseries prediction is just an extension to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll analyse the data and propose some simple baselines. We'll use **iterative prediction**, also known as walk-forward, to predict the next step at each timestep. The first baseline we'll use is called the naive baseline, which simply uses the previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation\n",
    "history = np.append(train_signal, np.zeros(len(test_signal)))\n",
    "predictions = np.zeros(len(test_signal))\n",
    "for i in range(len(test_signal)):\n",
    "    t = len(train_signal) + i\n",
    "    # make prediction\n",
    "    predictions[i] = history[t-1]\n",
    "    # observation\n",
    "    history[t] = test_signal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(predictions, test_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(test_signal[:512], label='test')\n",
    "plt.plot(predictions[:512], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a horizon of 1, the naive baseline is pretty good. Let's see if the historical data beyond one timestep can be useful. We'll start with a windowed approach, using the average of the past `W=5` timesteps as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.append(train_signal, np.zeros(len(test_signal)))\n",
    "predictions = np.zeros(len(test_signal))\n",
    "for i in range(len(test_signal)):\n",
    "    t = len(train_signal) + i\n",
    "    # make prediction\n",
    "    predictions[i] = np.mean(history[t-5:t])\n",
    "    # observation\n",
    "    history[t] = test_signal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(predictions, test_signal)))\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(test_signal[:512], label='test')\n",
    "plt.plot(predictions[:512], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this captures the general trend of some parts of the data, the naive baseline is still superior in terms of root mean squared error. Another approach is to exponentially decrease the dependency on past predictions, known as exponential smoothing. The rate by which we decrease past predictions is the parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(alpha):\n",
    "    history = np.append(train_signal, np.zeros(len(test_signal)))\n",
    "    predictions = np.zeros(len(test_signal))\n",
    "    for i in range(len(test_signal)):\n",
    "        t = len(train_signal) + i\n",
    "        # make prediction\n",
    "        if i == 0:\n",
    "            predictions[i] = history[t-1]\n",
    "        else:    \n",
    "            predictions[i] = alpha * history[t-1] + (1 - alpha) * predictions[i-1]\n",
    "        # observation\n",
    "        history[t] = test_signal[i]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_01 = exponential_smoothing(0.1)\n",
    "print(np.sqrt(mean_squared_error(predictions_01, test_signal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_05 = exponential_smoothing(0.5)\n",
    "np.sqrt(mean_squared_error(predictions_05, test_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(test_signal[:512], label='test')\n",
    "plt.plot(predictions_01[:512], label='prediction, alpha=0.1')\n",
    "plt.plot(predictions_05[:512], label='prediction, alpha=0.5')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While lower alpha values help predict the general trend of our data, their RMSE is worse than using very short history. So for this EEG data, we're still struggling to make good use of the historical data to predict future data. Instead of a single parameter for history decay, we'll instead use a recurrent neural network to inform our reliance on memory for prediction, and optimize the network parameters using Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 1:\n",
    "Apply these baselines to the `monthly-car-sales.csv` file, which records monthly car sales in Quebec, Canada between 1960 and 1968.\n",
    "\n",
    "+ Is this data stationary?\n",
    "+ What is a good alpha to use with this data?\n",
    "+ This data is monthly; is there any naive baseline better than taking the previous day's value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple recurrent neural network layer is very similar to a fully-connected feed-forward neural network layer; it has a set of weights $W_x$ mapping the previous layer $x$ to each neuron of the recurrent layer, a bias term for each neuron, and an activation function. However, a recurrent neural network also has state; specifically, each neuron connects to every other neuron in the same layer with a time delay of 1. This means that a recurrent neural network layer has a second weight matrix $W_s$ of size $n$x$n$, where $n$ is the number of neurons in the recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to consider these recurrent connections is by representing the previous activation functions of the recurrent layer as a hidden state, and using that hidden state as input to the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that in PyTorch, using the following scheme. We'll skip the softmax layer, as we only have 1 output:\n",
    "<img src='img/linear_rnn.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "rnn = RNN(1, n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = rnn.init_hidden()\n",
    "input = torch.from_numpy(np.array([[train_signal[-1]]])).float()\n",
    "print(input)\n",
    "output, hidden = rnn.forward(input, hidden)\n",
    "print(output)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with recurrent neural networks is that they depend on the previous state for the current state's computation. Instead of simply prediction $Y = f(x)$ as in feed-forward neural networks, recurrent networks do $Y_1 = f(x_1, f(x_0))$. Here's what an \"unrolled\" RNN looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first show this iterative computation, doing each step at a time. Note that PyTorch can do this computation **directly**, meaning predicting $Y = f(x)$ as long as $Y$ and $x$ are properly formatted. For now, we'll step through each sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.append(train_signal, np.zeros(len(test_signal)))\n",
    "predictions = np.zeros(len(test_signal))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_signal)):\n",
    "        t = len(train_signal) + i\n",
    "        # make prediction\n",
    "        input = torch.from_numpy(np.array([[history[t-1]]])).float()\n",
    "        output, hidden = rnn.forward(input, hidden)\n",
    "        predictions[i] = output.detach().numpy()[0][0]\n",
    "        # observation\n",
    "        history[t] = test_signal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(predictions, test_signal)))\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(test_signal[:512], label='test')\n",
    "plt.plot(predictions[:512], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network doesn't do very well, but it's using random weights. In order to train it, we'll need to calcuate the gradient throughout the iterative process. This is known as **backpropogation through time**, and it relies on the computation of not just the current timestep, but all previous timesteps as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bptt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(1, n_hidden, 1)\n",
    "hidden = rnn.init_hidden()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.001)\n",
    "for t in range(1, 1001):\n",
    "    optimizer.zero_grad()\n",
    "    input = torch.from_numpy(np.array([[train_signal[t-1]]])).float()\n",
    "    output, hidden = rnn.forward(input, hidden)\n",
    "    label = torch.from_numpy(np.array([[train_signal[t]]])).float()\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    if t % 100 == 0:\n",
    "        print('%d %0.3f %0.3f %0.3f' % (t, label.item(), output.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.append(train_signal, np.zeros(len(test_signal)))\n",
    "predictions = np.zeros(len(test_signal))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_signal)):\n",
    "        t = len(train_signal) + i\n",
    "        # make prediction\n",
    "        input = torch.from_numpy(np.array([[history[t-1]]])).float()\n",
    "        output, hidden = rnn.forward(input, hidden)\n",
    "        predictions[i] = output.detach().numpy()[0][0]\n",
    "        # observation\n",
    "        history[t] = test_signal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(predictions, test_signal)))\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(test_signal[:512], label='test')\n",
    "plt.plot(predictions[:512], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 2:\n",
    "    Create a model using the PyTorch `RNN` class to train on the full training set, split up into minibatches. Test your prediction on the test set. Tweak the number of RNN layers, the size of the hidden layers, and the optimizer (RMS and LBFGS are common choices for RNNs). These two examples are good guides:\n",
    "\n",
    "+ https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb\n",
    "+ https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex recurrent neural network, which actually predates much of the current era of deep learning, is called the Long Short-Term Memory unit. This unit was designed to have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/lstm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1}+b_i)\\\\\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1}+b_f)\\\\\n",
    "c_t = f_tc_{t-1}+i_t\\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\\\\\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t-1}+b_o)\\\\\n",
    "h_t = o_t\\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 3\n",
    "\n",
    "Modify your model from part exercise 2 to use the PyTorch `LSTM` or `LSTMCell` classes. Compare your test prediction error against the RNN model and against the naive baseline. Note that with LSTMs, and recurrent neural networks in general, gradients can tend to either vanish or grow very large (explode). A simple remedy to exploding gradients is using gradient clipping: `torch.nn.utils.clip_grad`.\n",
    "    \n",
    "Another recurrent layer type is the Gated Recurrent Unit. It was designed to solve the issue of vanishing gradients in LSTMs. [Here](https://arxiv.org/pdf/1412.3555.pdf) is an empirical study of the layer types. Try replacing your LSTM with a GRU to see if it changes your results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to know more about time-series prediction, [this notebook](https://github.com/marcopeix/stock-prediction/blob/master/Stock%20Prediction.ipynb) gives an example of more complex classical prediction models, notably Seasonal ARIMA.\n",
    "\n",
    "In the next class, we'll see one of the most popular uses of LSTMs currently, in the context of [Natural Language Processing](https://arxiv.org/pdf/1810.04805.pdf). Before next class, please make sure you've installed `rasa`, `gensim`, and `nest-asyncio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
