{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Creating Chatbots and recognizing entities</div>\n",
    "\n",
    "In this exercice, we will apply the example of text classification studied previously to the creation of chatbots.\n",
    "\n",
    "Building an agent capable of handling conversations is usually performed through the following steps:\n",
    "1. We apply text classification techniques to the utterance typed by the user, in order to detect the **intent** of the user (e.g. \"applying for a loan\", \"purchasing a product from a store\", \"asking about the weather\", etc)\n",
    "2. In addition to detecting intent, we extract **Named Entities** from the utterance (e.g. location names, currency amounts, etc...)\n",
    "3. We use the extracted intents and entities to decide how the chatbot should reply\n",
    "\n",
    "In this exercise, we will use the open source chatbot framework **Rasa** to build the chatbot. It includes the package *Rasa NLU* that performs NLU tasks such as entity extraction and intent classification, as well as *Rasa Core*, that manages the conversational aspects.\n",
    "\n",
    "# 1. Using Rasa\n",
    "\n",
    "Rasa runs code asynchronously. The following cell configures the notebook for Rasa use. See [the documentation of Rasa](https://rasa.com/docs/rasa/api/jupyter-notebooks/) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Rasa will throw plenty of warnings during execution. This line gets rid of them, for lisibility. \n",
    "# Just comment it to keep the warnings if desired\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa chatbot project folders follow a precise structure to function. Important files to consider are:\n",
    "1. A **config.yml** file defining the language of the chatbot, the methods used for intent classification, etc...\n",
    "2. A **domain.yml** file, defining the intents and entities covered by the chatbot, as well as the utterances and actions it will use. Generally, most of the engineering of chatbots goes towards this file.\n",
    "3. A **nlu** file (Rasa uses json or md files), containing the training samples for intent classification and entity extraction.\n",
    "4. A **stories** file, contaning the different scenariis of interaction between the user and the bot. This file defines how to associate bot actions and intents/entities.\n",
    "\n",
    "When starting a new project, Rasa provides a useful function that prepares the folder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa.cli.scaffold import create_initial_project\n",
    "\n",
    "project = \"my_new_chatbot_project\"\n",
    "create_initial_project(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building our first chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/1_simple_clockbot/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first chatbot is meant to be a simple clockbot (a chatbot that gives the time). Let's go through the structure of the files to perform this task.\n",
    "\n",
    "## 2.a. config file\n",
    "\n",
    "```\n",
    "# Configuration for Rasa NLU.\n",
    "# https://rasa.com/docs/rasa/nlu/components/\n",
    "language: fr\n",
    "pipeline: supervised_embeddings\n",
    "```\n",
    "\n",
    "The chatbot is meant to be in french. The pipeline we use is the default Rasa one, **supervised_embeddings**.\n",
    "\n",
    "In the text classification examples we saw in the previous exercise, we learnt individual vector representations of every word in our corpus, and used these as features for classification. We saw that it causes issues with out-of-vocabulary (OOV) words, and with model storage. The main modification to embeddings proposed by Rasa is to learn intent representations as ell as words'. This means that any input sequence can be judged in terms of similarity with an intent, making it easier and less costly to perform classification. You can read more about this [here](https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8)\n",
    "\n",
    "## 2.b. domain file\n",
    "\n",
    "```\n",
    "intents:\n",
    "  - greet\n",
    "  - ask_time\n",
    "\n",
    "actions:\n",
    "- utter_greet\n",
    "- utter_time\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "  - text: \"Bien le bonjour !\"\n",
    "  utter_time:\n",
    "  - text: \"Il est 10h\"\n",
    "```\n",
    "\n",
    "As introduced earlier, this first chatbot will:\n",
    "- cover 2 intents (greeting and asking what time it is)\n",
    "- perform only two possible actions (greet the user back, or give the time)\n",
    "\n",
    "Both actions performed by the chatbot in this example are single utterances, defined within the file itself. It would be possible to declare more complex actions, ran separately in an \"action server\" (see [the documentation](https://rasa.com/docs/rasa/core/actions/#actions)). As the objective of this exercise is not to define an action server, but rather to explore machine learning capabilities of chatbots, we propose to simplify the problem by creating a dumb clockbot stuck at 10 o'clock.\n",
    "\n",
    "## 2.c. stories file\n",
    "\n",
    "```\n",
    "## greet path\n",
    "* greet\n",
    "  - utter_greet\n",
    "\n",
    "## time_path_polite\n",
    "* greet\n",
    "  - utter_greet\n",
    "* ask_time\n",
    "  - utter_time\n",
    "  - action_restart\n",
    "```\n",
    "\n",
    "We define only two main scenarii for interaction with this chatbot :\n",
    "- the user greets the chatbot, but interacts no further\n",
    "- the user greets the chatbot, then asks what time it is (the action_restart at the end of this path reinitializes the bot once it has given the time)\n",
    "\n",
    "Note that we have not defined a scenario where the user would ask the time without greeting the robot first (we will cover this in the next exercise).\n",
    "\n",
    "## 2.d. nlu file\n",
    "\n",
    "```\n",
    "## intent:greet\n",
    "- salut\n",
    "- hello\n",
    "- yo\n",
    "- slt\n",
    "- bonjour\n",
    "- bonsoir\n",
    "\n",
    "## intent:ask_time\n",
    "- il est quelle heure ?\n",
    "- quelle heure est-il\n",
    "- t'as l'heure stp ?\n",
    "```\n",
    "\n",
    "This file contains examples for all the intents that we have defined for the chatbot. Note that we included some examples containing abbreviations, mispellings, etc... Even if NLU models are designed to be robust to misspellings when trained on correctly spelled utterances, it is a good practice to include examples \"from the real world\" for each intent, and to include them as such (without correcting misspellings).\n",
    "\n",
    "## 2.e. Training the chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTraining Core model...\u001b[0m\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\utils\\adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\utils\\multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:Failed to load tensor2tensor\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\models\\research\\glow_init_hook.py:25: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\models\\research\\neural_stack.py:51: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\utils\\trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensor2tensor\\utils\\trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 502.85it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 334.18it/s, # trackers=2]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 334.09it/s, # trackers=4]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 222.83it/s, # trackers=6]\n",
      "Processed trackers: 100%|███████████████████████████████████████████████████| 2/2 [00:00<00:00, 95.48it/s, # actions=6]\n",
      "Processed actions: 6it [00:00, 501.31it/s, # examples=6]\n",
      "Processed trackers: 100%|████████████████████████████████████████████████| 20/20 [00:00<00:00, 83.90it/s, # actions=18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\core\\policies\\keras_policy.py:184: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\core\\policies\\keras_policy.py:184: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 5, 13)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                5888      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11)                363       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 6,251\n",
      "Trainable params: 6,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 18 samples\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 43ms/sample - loss: 2.3509 - acc: 0.2222\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 2.3357 - acc: 0.3333\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 2.2976 - acc: 0.6111\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 2.2775 - acc: 0.4444\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 2.2616 - acc: 0.3889\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 2.2360 - acc: 0.6111\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 554us/sample - loss: 2.2533 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 2.1944 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 2.1916 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 2.1822 - acc: 0.4444\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 2.1353 - acc: 0.5556\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 2.1350 - acc: 0.4444\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 2.1376 - acc: 0.4444\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 2.0972 - acc: 0.6111\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 2.1045 - acc: 0.3889\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 2.0427 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 2.0711 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 2.0410 - acc: 0.4444\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.9871 - acc: 0.4444\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.9641 - acc: 0.4444\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 444us/sample - loss: 1.9686 - acc: 0.4444\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 1.9603 - acc: 0.4444\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.9775 - acc: 0.4444\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.9045 - acc: 0.4444\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.9390 - acc: 0.4444\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.8623 - acc: 0.4444\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.8641 - acc: 0.4444\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 1.9190 - acc: 0.4444\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.8118 - acc: 0.4444\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.7791 - acc: 0.4444\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.7613 - acc: 0.4444\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.7416 - acc: 0.4444\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 610us/sample - loss: 1.8107 - acc: 0.4444\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.7397 - acc: 0.3889\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.7225 - acc: 0.3889\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.7500 - acc: 0.4444\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 610us/sample - loss: 1.7300 - acc: 0.4444\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 498us/sample - loss: 1.7111 - acc: 0.4444\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.6438 - acc: 0.4444\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 442us/sample - loss: 1.6213 - acc: 0.3889\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.6260 - acc: 0.3889\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 498us/sample - loss: 1.6167 - acc: 0.3889\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.5685 - acc: 0.4444\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 554us/sample - loss: 1.5612 - acc: 0.4444\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.6208 - acc: 0.4444\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.5561 - acc: 0.4444\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 333us/sample - loss: 1.5316 - acc: 0.4444\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 277us/sample - loss: 1.5732 - acc: 0.4444\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.5283 - acc: 0.4444\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 887us/sample - loss: 1.5250 - acc: 0.4444\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 664us/sample - loss: 1.4855 - acc: 0.4444\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.5215 - acc: 0.4444\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 444us/sample - loss: 1.4979 - acc: 0.3889\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.4844 - acc: 0.3889\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.4823 - acc: 0.4444\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.4943 - acc: 0.4444\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.4301 - acc: 0.4444\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.4173 - acc: 0.4444\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.4539 - acc: 0.4444\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 333us/sample - loss: 1.4485 - acc: 0.4444\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.3765 - acc: 0.4444\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.4246 - acc: 0.4444\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.4165 - acc: 0.4444\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.4129 - acc: 0.5000\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.3775 - acc: 0.5000\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.4053 - acc: 0.5556\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 2ms/sample - loss: 1.3698 - acc: 0.4444\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 720us/sample - loss: 1.3526 - acc: 0.4444\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 665us/sample - loss: 1.3897 - acc: 0.4444\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.3562 - acc: 0.5556\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.3481 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 720us/sample - loss: 1.3491 - acc: 0.4444\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.3538 - acc: 0.4444\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.3281 - acc: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.3453 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.3631 - acc: 0.5000\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.3207 - acc: 0.5000\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.3444 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.3180 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.2721 - acc: 0.5556\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 500us/sample - loss: 1.2999 - acc: 0.5556\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 776us/sample - loss: 1.2924 - acc: 0.6111\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 0s 609us/sample - loss: 1.2633 - acc: 0.6111\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 444us/sample - loss: 1.2602 - acc: 0.5000\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.2541 - acc: 0.6667\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 720us/sample - loss: 1.2976 - acc: 0.6667\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.2841 - acc: 0.6111\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.2660 - acc: 0.6111\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 332us/sample - loss: 1.2577 - acc: 0.6667\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 831us/sample - loss: 1.2487 - acc: 0.6111\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.2165 - acc: 0.6111\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.1964 - acc: 0.6667\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.2347 - acc: 0.6111\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.2506 - acc: 0.6667\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 554us/sample - loss: 1.2359 - acc: 0.5556\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.2039 - acc: 0.7778\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 388us/sample - loss: 1.1957 - acc: 0.6667\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 499us/sample - loss: 1.1769 - acc: 0.7778\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 443us/sample - loss: 1.1789 - acc: 0.7222\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 553us/sample - loss: 1.2139 - acc: 0.5556\n",
      "\u001b[94mCore model training completed.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n",
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:736: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:736: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:518: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:518: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:519: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:519: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:519: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:519: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:750: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:750: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:548: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:548: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:548: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:548: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:550: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:550: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:559: The name tf.sparse.matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:559: The name tf.sparse.matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:603: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:603: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:605: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:605: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:984: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:984: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:986: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:986: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:758: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:758: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:1168: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:1168: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epochs: 100%|█████████████████████████████████████████████████| 300/300 [00:31<00:00,  9.65it/s, loss=0.544, acc=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:882: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\nlu\\classifiers\\embedding_intent_classifier.py:882: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mNLU model training completed.\u001b[0m\n",
      "\u001b[92mYour Rasa model is trained and saved at 'C:\\Users\\DURANTGA\\Documents\\Pédagogie\\supaero\\deep-learning\\NLP\\chatbot_projects\\1_simple_clockbot\\models\\20211201-143146.tar.gz'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import rasa\n",
    "\n",
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.f. Using the chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpwuqur4sr\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpwuqur4sr\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:1269: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\rasa\\utils\\train_utils.py:1269: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\durantga\\appdata\\local\\continuum\\anaconda3\\envs\\courssup\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your bot is ready to talk! Type your messages here or send '/stop'.\n",
      "salut\n",
      "\u001b[92mBien le bonjour !\u001b[0m\n",
      "ca va ?\n",
      "\u001b[92mBien le bonjour !\u001b[0m\n",
      "exit\n",
      "/stop\n"
     ]
    }
   ],
   "source": [
    "from rasa.jupyter import chat\n",
    "chat(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.g. Checking the behaviour of the bot classifier\n",
    "\n",
    "In this section, we will cofirm the resilience of the bot to mistakes during typing.\n",
    "We first load the model in to an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpbrw8kp45\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpbrw8kp45\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    }
   ],
   "source": [
    "from rasa.core.agent import Agent\n",
    "agent=Agent.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following code, we can then parse various messages and see how the nlu classifier ranks them.\n",
    "We can check that :\n",
    "- standard messages for the intents (e.g. \"salut\") get classified correctly, with very high confidence levels\n",
    "- messages that vary quite extensively from the canonial question used during training can sill get classified correctly, but with lower confidence levels (e.g. using sms-style \"kel h?\")\n",
    "- if the messages differ too much from the samples used during training, the confidence level will decrease dramatically. \n",
    "\n",
    "**WARNING** : This example can be biased, as we are using only two intents (classes) with very different syntax (greetings are mostly single words, while more complex sentences are used to ask for time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'greet', 'confidence': 0.99524986743927},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'greet', 'confidence': 0.99524986743927},\n",
       "  {'name': 'ask_time', 'confidence': 0.004750147461891174}],\n",
       " 'text': 'salut'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"salut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.7192785739898682},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.7192785739898682},\n",
       "  {'name': 'greet', 'confidence': 0.28072142601013184}],\n",
       " 'text': 'kel h'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'greet', 'confidence': 0.6506250500679016},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'greet', 'confidence': 0.6506250500679016},\n",
       "  {'name': 'ask_time', 'confidence': 0.349374920129776}],\n",
       " 'text': 'compte'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"compte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Adding different paths\n",
    "\n",
    "In the previous example, we did not consider the case when a user would ask what time it is without greeting the bot first. In this example, we propose to adapt the scenarii of user interaction to handle those cases.\n",
    "\n",
    "The main modification is located in the **stories** file, where the following story is added :\n",
    "```\n",
    "## time_path_impolite\n",
    "* ask_time\n",
    "  - utter_impolite\n",
    "```\n",
    "\n",
    "Note that an action (utterance) has been added to handle the situation where the bot gets offended because the user did not greet it. This is also reflected in the **domain.yml** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/2_obnoxious_clockbot/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our bot and run it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 601.62it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 273.47it/s, # trackers=3]\n",
      "Processed Story Blocks: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 176.91it/s, # trackers=12]\n",
      "Processed Story Blocks: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 111.40it/s, # trackers=24]\n",
      "Processed trackers:   0%|                                                           | 0/3 [00:00<?, ?it/s, # actions=7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTraining Core model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed trackers: 100%|██████████████████████████████████████████████████| 3/3 [00:00<00:00, 176.99it/s, # actions=7]\n",
      "Processed actions: 7it [00:00, 438.63it/s, # examples=7]\n",
      "Processed trackers: 100%|██████████████████████████████████████████████| 102/102 [00:01<00:00, 64.64it/s, # actions=49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 5, 14)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                6016      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 6,412\n",
      "Trainable params: 6,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 12ms/sample - loss: 2.4761 - acc: 0.0204\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 2.4253 - acc: 0.1837\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 2.3870 - acc: 0.2449\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 2.3618 - acc: 0.3673\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 2.3135 - acc: 0.4490\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 2.2799 - acc: 0.4082\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 2.2545 - acc: 0.4898\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 2.2204 - acc: 0.4694\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 2.1837 - acc: 0.4490\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 2.1531 - acc: 0.5102\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 2.1365 - acc: 0.4898\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 285us/sample - loss: 2.0602 - acc: 0.5102\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 2.0153 - acc: 0.4898\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 326us/sample - loss: 1.9597 - acc: 0.5102\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 509us/sample - loss: 1.9043 - acc: 0.5102\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 305us/sample - loss: 1.9047 - acc: 0.5102\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 1.8229 - acc: 0.5102\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.7844 - acc: 0.5102\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.7538 - acc: 0.5102\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.6470 - acc: 0.5102\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.6283 - acc: 0.5102\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.6040 - acc: 0.5102\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.5341 - acc: 0.5102\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.5228 - acc: 0.5102\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.5112 - acc: 0.5102\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 550us/sample - loss: 1.4567 - acc: 0.5102\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.4018 - acc: 0.5102\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.4018 - acc: 0.5102\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.3939 - acc: 0.5102\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.3530 - acc: 0.5102\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.3679 - acc: 0.5102\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.3477 - acc: 0.5102\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.3533 - acc: 0.5102\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 1.3233 - acc: 0.5102\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.3058 - acc: 0.5102\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.3304 - acc: 0.5102\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.2943 - acc: 0.5102\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.2652 - acc: 0.5102\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.2763 - acc: 0.5102\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.2682 - acc: 0.5102\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.2460 - acc: 0.5102\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.2403 - acc: 0.5102\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.2308 - acc: 0.5102\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.2269 - acc: 0.5102\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 1.2357 - acc: 0.5102\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 1.2256 - acc: 0.5102\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.2026 - acc: 0.5102\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.2129 - acc: 0.5102\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 1.2111 - acc: 0.5102\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.1953 - acc: 0.5306\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 611us/sample - loss: 1.1938 - acc: 0.5306\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 509us/sample - loss: 1.1887 - acc: 0.5102\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 611us/sample - loss: 1.1799 - acc: 0.5306\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 488us/sample - loss: 1.1591 - acc: 0.5306\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 509us/sample - loss: 1.1849 - acc: 0.5102\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 1.1603 - acc: 0.5306\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.1474 - acc: 0.5306\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 651us/sample - loss: 1.1528 - acc: 0.5102\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.1358 - acc: 0.5306\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 550us/sample - loss: 1.1426 - acc: 0.5306\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.1435 - acc: 0.5306\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 570us/sample - loss: 1.1176 - acc: 0.5306\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 1.1069 - acc: 0.5306\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.1066 - acc: 0.5306\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1330 - acc: 0.468 - 0s 427us/sample - loss: 1.1230 - acc: 0.5306\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.1070 - acc: 0.5306\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 1.0999 - acc: 0.5306\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 489us/sample - loss: 1.1057 - acc: 0.5102\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 753us/sample - loss: 1.0822 - acc: 0.5306\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 672us/sample - loss: 1.0755 - acc: 0.5306\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 1.0835 - acc: 0.5306\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.0738 - acc: 0.5306\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 631us/sample - loss: 1.0415 - acc: 0.5306\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 611us/sample - loss: 1.0546 - acc: 0.5306\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 1.0559 - acc: 0.5306\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 1.0543 - acc: 0.5306\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.0445 - acc: 0.5306\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.0273 - acc: 0.5306\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 1.0188 - acc: 0.5306\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 1.0163 - acc: 0.5306\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 326us/sample - loss: 1.0186 - acc: 0.5306\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 1.0131 - acc: 0.5306\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 1.0051 - acc: 0.5306\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 0.9902 - acc: 0.5306\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 0.9819 - acc: 0.5306\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 0.9658 - acc: 0.5306\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 366us/sample - loss: 0.9509 - acc: 0.5510\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 0.9382 - acc: 0.5306\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 0.9379 - acc: 0.5306\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 448us/sample - loss: 0.9448 - acc: 0.5306\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 346us/sample - loss: 0.9168 - acc: 0.5918\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 468us/sample - loss: 0.9261 - acc: 0.5714\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 529us/sample - loss: 0.9167 - acc: 0.6327\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 488us/sample - loss: 0.8916 - acc: 0.6531\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 550us/sample - loss: 0.8752 - acc: 0.6939\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 509us/sample - loss: 0.8701 - acc: 0.7347\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 427us/sample - loss: 0.8592 - acc: 0.7143\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 407us/sample - loss: 0.8668 - acc: 0.7347\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 387us/sample - loss: 0.8688 - acc: 0.7347\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 489us/sample - loss: 0.8372 - acc: 0.7347\n",
      "\u001b[94mCore model training completed.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████████████████████████████████████████████████| 300/300 [00:30<00:00,  9.77it/s, loss=0.586, acc=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mNLU model training completed.\u001b[0m\n",
      "\u001b[92mYour Rasa model is trained and saved at 'C:\\Users\\DURANTGA\\Documents\\Pédagogie\\supaero\\deep-learning\\NLP\\chatbot_projects\\2_obnoxious_clockbot\\models\\20211201-143302.tar.gz'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells below run the polite scenario (greeting + asking for time) and the impolite one (asking for time directly).\n",
    "We confirm here that the bot behaves differently in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpii2isy1p\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpii2isy1p\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'default', 'text': 'Bien le bonjour !'}]\n",
      "[{'recipient_id': 'default', 'text': 'Il est 10h'}]\n"
     ]
    }
   ],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"slt\"))\n",
    "print(await agent.handle_text(\"il est kel h ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmp4og6nv5a\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmp4og6nv5a\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'default', 'text': 'Tu pourrais au moins me dire bonjour !'}]\n"
     ]
    }
   ],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"il est kel h ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding entities\n",
    "\n",
    "In this example, we will add **Named Entity extraction** to the bot. \n",
    "\n",
    "Named Entities can be viewed as instances of linguistic classes. For instance, there is an inifinite number of ways through which one might refer to a Location. Some examples could be \"Paris\", \"my house\", \"Japan\", \"the restaurant I like on Main Street\", etc... From a Named Entity Recognition (NER) perspective, all these examples would be specific **values** (or synonyms) refering to the **entity** \"location\".\n",
    "\n",
    "## 4.a How are entities extracted by Rasa ?\n",
    "\n",
    "### Symbolic approach\n",
    "\n",
    "The simplest way of approaching Named Entity Extraction is to define all synonyms of an entity manually. In this case, the user would type all possible values of an entity manually. The extraction of these entities would then be performed by matching words in an utterance with the list of synonyms.\n",
    "\n",
    "There are some cases where this mode of definition is appropriate. For instance, to define the entity \"*means of payment*\", there are only limited options (*cash*,*cheque*,*credit card*,*debit card*...). This approach, however, has two main issues :\n",
    "- the maintenance of synonym lists can quickly become impossible (for instance when defining entities corresponding to person or loction names)\n",
    "- this approach is not resilient to spelling variants (e.g. typing \"crdt card\" instead of \"credit card\"). \n",
    "\n",
    "The second issue can be mitigated by introducing \"flexible\" or \"fuzzy\" matching for entities (i.e. considering that a word that differs by only one character from one of the synonyms is still a match). However, in many cases, this is still imperfect.\n",
    "\n",
    "### Statistical approach\n",
    "\n",
    "Rasa relies on **extractors** for entities. For instance, the default CRFExtractor uses Conditional Random Fields fitted on word embeddings to predict whether a given word is an entity. In practice, that means that entities are detected using their representation, as well as the representations of the words in their surrounding.  \n",
    "\n",
    "In our case, using this approach means that giving enough examples in the form \"Quelle heure est-il à Paris\", \"quelle heure est-il à Londres\", etc... will enable the extractor to understand that in the pattern \"quelle heure est-il à ....\", the word following \"à\" is a \"location\".\n",
    "\n",
    "## 4.b. Defining location entities for our chatbot\n",
    "\n",
    "We propose to add the capacity to ask what the time is at a given location to our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/3_obnoxious_clockbot_with_entities/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial modification is located in the **domain** file, where the entity type \"*location*\" is defined. We also use the value of this entity in the \"*utter_time*\" utterance.\n",
    "```\n",
    "entities:\n",
    "  - location\n",
    "\n",
    "[...]\n",
    "\n",
    "  utter_time:\n",
    "  - text: \"Il est 10h à {location}\"\n",
    "```\n",
    "\n",
    "The training data in the **nlu** file is also modified to reflect this change :\n",
    "\n",
    "```\n",
    "## intent:ask_time\n",
    "- il est quelle heure à [New York](location)\n",
    "- quelle heure est-il à [Paris](location)\n",
    "- quelle heure est-il à [Séoul](location)\n",
    "- t'as l'heure de [Tokyo](location) stp ?\n",
    "- heure de [Lyon](location)\n",
    "```\n",
    "\n",
    "In the above examples, the sentence ```Quelle heure est-il à [Lyon](location)``` should be read as \"*Quelle heure est-il à Lyon*\", but where Lyon is an instance of the \"location\" entity.\n",
    "More information about the format of training data can be found in [the documentation](https://rasa.com/docs/rasa/nlu/training-data-format/).\n",
    "\n",
    "## 4.c. Training and testing the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 602.08it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 250.67it/s, # trackers=3]\n",
      "Processed Story Blocks: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 158.32it/s, # trackers=12]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 88.50it/s, # trackers=28]\n",
      "Processed trackers:   0%|                                                           | 0/3 [00:00<?, ?it/s, # actions=4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTraining Core model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed trackers: 100%|██████████████████████████████████████████████████| 3/3 [00:00<00:00, 200.55it/s, # actions=7]\n",
      "Processed actions: 7it [00:00, 702.08it/s, # examples=7]\n",
      "Processed trackers: 100%|██████████████████████████████████████████████| 110/110 [00:01<00:00, 61.41it/s, # actions=73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 5, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 6,668\n",
      "Trainable params: 6,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 73 samples\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 9ms/sample - loss: 2.4127 - acc: 0.2192\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 2.3037 - acc: 0.3562\n",
      "Epoch 3/100\n",
      "73/73 [==============================] - 0s 478us/sample - loss: 2.2190 - acc: 0.4110\n",
      "Epoch 4/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 2.1392 - acc: 0.4521\n",
      "Epoch 5/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 2.0899 - acc: 0.4384\n",
      "Epoch 6/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 2.0180 - acc: 0.4521\n",
      "Epoch 7/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 1.9598 - acc: 0.5205\n",
      "Epoch 8/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 1.8733 - acc: 0.4795\n",
      "Epoch 9/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.8029 - acc: 0.5616\n",
      "Epoch 10/100\n",
      "73/73 [==============================] - ETA: 0s - loss: 1.7944 - acc: 0.562 - 0s 342us/sample - loss: 1.7379 - acc: 0.5068\n",
      "Epoch 11/100\n",
      "73/73 [==============================] - 0s 287us/sample - loss: 1.6966 - acc: 0.5068\n",
      "Epoch 12/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 1.6492 - acc: 0.5205\n",
      "Epoch 13/100\n",
      "73/73 [==============================] - 0s 451us/sample - loss: 1.5488 - acc: 0.5205\n",
      "Epoch 14/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 1.5399 - acc: 0.5205\n",
      "Epoch 15/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.5234 - acc: 0.5205\n",
      "Epoch 16/100\n",
      "73/73 [==============================] - 0s 342us/sample - loss: 1.4439 - acc: 0.5205\n",
      "Epoch 17/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 1.3999 - acc: 0.5205\n",
      "Epoch 18/100\n",
      "73/73 [==============================] - 0s 301us/sample - loss: 1.4177 - acc: 0.5068\n",
      "Epoch 19/100\n",
      "73/73 [==============================] - 0s 465us/sample - loss: 1.3692 - acc: 0.5205\n",
      "Epoch 20/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 1.3405 - acc: 0.5342\n",
      "Epoch 21/100\n",
      "73/73 [==============================] - 0s 492us/sample - loss: 1.3401 - acc: 0.5205\n",
      "Epoch 22/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.3191 - acc: 0.5205\n",
      "Epoch 23/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.3268 - acc: 0.5342\n",
      "Epoch 24/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.3060 - acc: 0.5342\n",
      "Epoch 25/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 1.2845 - acc: 0.5342\n",
      "Epoch 26/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 1.2725 - acc: 0.5342\n",
      "Epoch 27/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.2595 - acc: 0.5205\n",
      "Epoch 28/100\n",
      "73/73 [==============================] - 0s 342us/sample - loss: 1.2460 - acc: 0.5205\n",
      "Epoch 29/100\n",
      "73/73 [==============================] - 0s 465us/sample - loss: 1.2608 - acc: 0.5205\n",
      "Epoch 30/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.2423 - acc: 0.5205\n",
      "Epoch 31/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 1.2266 - acc: 0.5342\n",
      "Epoch 32/100\n",
      "73/73 [==============================] - 0s 451us/sample - loss: 1.2250 - acc: 0.5342\n",
      "Epoch 33/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 1.2081 - acc: 0.5342\n",
      "Epoch 34/100\n",
      "73/73 [==============================] - 0s 519us/sample - loss: 1.2135 - acc: 0.5479\n",
      "Epoch 35/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 1.1920 - acc: 0.5616\n",
      "Epoch 36/100\n",
      "73/73 [==============================] - 0s 560us/sample - loss: 1.2129 - acc: 0.5479\n",
      "Epoch 37/100\n",
      "73/73 [==============================] - 0s 301us/sample - loss: 1.1749 - acc: 0.5479\n",
      "Epoch 38/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.1683 - acc: 0.5479\n",
      "Epoch 39/100\n",
      "73/73 [==============================] - 0s 519us/sample - loss: 1.1605 - acc: 0.5616\n",
      "Epoch 40/100\n",
      "73/73 [==============================] - 0s 423us/sample - loss: 1.1574 - acc: 0.5342\n",
      "Epoch 41/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 1.1446 - acc: 0.5890\n",
      "Epoch 42/100\n",
      "73/73 [==============================] - ETA: 0s - loss: 1.2789 - acc: 0.593 - 0s 342us/sample - loss: 1.1450 - acc: 0.5753\n",
      "Epoch 43/100\n",
      "73/73 [==============================] - 0s 342us/sample - loss: 1.1389 - acc: 0.5890\n",
      "Epoch 44/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 1.1312 - acc: 0.5479\n",
      "Epoch 45/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 1.1241 - acc: 0.5890\n",
      "Epoch 46/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 1.1192 - acc: 0.6164\n",
      "Epoch 47/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.0915 - acc: 0.5616\n",
      "Epoch 48/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 1.0846 - acc: 0.6164\n",
      "Epoch 49/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.0853 - acc: 0.5890\n",
      "Epoch 50/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 1.0873 - acc: 0.5753\n",
      "Epoch 51/100\n",
      "73/73 [==============================] - 0s 451us/sample - loss: 1.0724 - acc: 0.6164\n",
      "Epoch 52/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 1.0645 - acc: 0.6027\n",
      "Epoch 53/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 1.0669 - acc: 0.6164\n",
      "Epoch 54/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 1.0373 - acc: 0.6027\n",
      "Epoch 55/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 1.0464 - acc: 0.6164\n",
      "Epoch 56/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 1.0475 - acc: 0.6164\n",
      "Epoch 57/100\n",
      "73/73 [==============================] - 0s 342us/sample - loss: 1.0350 - acc: 0.6301\n",
      "Epoch 58/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 1.0209 - acc: 0.6301\n",
      "Epoch 59/100\n",
      "73/73 [==============================] - 0s 478us/sample - loss: 1.0282 - acc: 0.5753\n",
      "Epoch 60/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 1.0021 - acc: 0.6712\n",
      "Epoch 61/100\n",
      "73/73 [==============================] - 0s 615us/sample - loss: 0.9899 - acc: 0.6575\n",
      "Epoch 62/100\n",
      "73/73 [==============================] - 0s 478us/sample - loss: 0.9944 - acc: 0.6575\n",
      "Epoch 63/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 0.9700 - acc: 0.6575\n",
      "Epoch 64/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 0.9745 - acc: 0.6575\n",
      "Epoch 65/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 0.9794 - acc: 0.6712\n",
      "Epoch 66/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.9685 - acc: 0.6712\n",
      "Epoch 67/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 0.9423 - acc: 0.6986\n",
      "Epoch 68/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 0.9345 - acc: 0.6986\n",
      "Epoch 69/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.9518 - acc: 0.6301\n",
      "Epoch 70/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 0.9230 - acc: 0.7534\n",
      "Epoch 71/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 0.9347 - acc: 0.6849\n",
      "Epoch 72/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.9044 - acc: 0.7397\n",
      "Epoch 73/100\n",
      "73/73 [==============================] - 0s 492us/sample - loss: 0.8988 - acc: 0.7397\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 424us/sample - loss: 0.8749 - acc: 0.7808\n",
      "Epoch 75/100\n",
      "73/73 [==============================] - 0s 437us/sample - loss: 0.8864 - acc: 0.7397\n",
      "Epoch 76/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 0.8652 - acc: 0.7808\n",
      "Epoch 77/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 0.8528 - acc: 0.7397\n",
      "Epoch 78/100\n",
      "73/73 [==============================] - 0s 465us/sample - loss: 0.8262 - acc: 0.7945\n",
      "Epoch 79/100\n",
      "73/73 [==============================] - 0s 424us/sample - loss: 0.8224 - acc: 0.7945\n",
      "Epoch 80/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.8183 - acc: 0.7671\n",
      "Epoch 81/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 0.8015 - acc: 0.7945\n",
      "Epoch 82/100\n",
      "73/73 [==============================] - 0s 342us/sample - loss: 0.7887 - acc: 0.8082\n",
      "Epoch 83/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 0.8228 - acc: 0.7671\n",
      "Epoch 84/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 0.8081 - acc: 0.7671\n",
      "Epoch 85/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 0.8128 - acc: 0.8082\n",
      "Epoch 86/100\n",
      "73/73 [==============================] - 0s 328us/sample - loss: 0.7725 - acc: 0.8219\n",
      "Epoch 87/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 0.7612 - acc: 0.8219\n",
      "Epoch 88/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 0.7563 - acc: 0.7808\n",
      "Epoch 89/100\n",
      "73/73 [==============================] - 0s 410us/sample - loss: 0.7549 - acc: 0.7671\n",
      "Epoch 90/100\n",
      "73/73 [==============================] - 0s 451us/sample - loss: 0.7535 - acc: 0.7808\n",
      "Epoch 91/100\n",
      "73/73 [==============================] - 0s 492us/sample - loss: 0.7146 - acc: 0.8493\n",
      "Epoch 92/100\n",
      "73/73 [==============================] - 0s 383us/sample - loss: 0.6997 - acc: 0.8904\n",
      "Epoch 93/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.6966 - acc: 0.7808\n",
      "Epoch 94/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.6875 - acc: 0.8493\n",
      "Epoch 95/100\n",
      "73/73 [==============================] - 0s 396us/sample - loss: 0.6737 - acc: 0.8493\n",
      "Epoch 96/100\n",
      "73/73 [==============================] - 0s 355us/sample - loss: 0.7037 - acc: 0.8219\n",
      "Epoch 97/100\n",
      "73/73 [==============================] - 0s 369us/sample - loss: 0.6368 - acc: 0.8493\n",
      "Epoch 98/100\n",
      "73/73 [==============================] - 0s 547us/sample - loss: 0.6636 - acc: 0.8630\n",
      "Epoch 99/100\n",
      "73/73 [==============================] - 0s 492us/sample - loss: 0.6577 - acc: 0.8219\n",
      "Epoch 100/100\n",
      "73/73 [==============================] - 0s 465us/sample - loss: 0.6276 - acc: 0.8767\n",
      "\u001b[94mCore model training completed.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|████████████████████████████████▉                | 202/300 [00:21<00:09, 10.16it/s, loss=0.647, acc=1.000]"
     ]
    }
   ],
   "source": [
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test the chatbot using a city name that was not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"salut\"))\n",
    "print(await agent.handle_text(\"il est quelle heure à Toulouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an individual utterance, we can check that the model has learned to recognize entities based on the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.9700797200202942},\n",
       " 'entities': [{'start': 6,\n",
       "   'end': 14,\n",
       "   'value': 'Toulouse',\n",
       "   'entity': 'location',\n",
       "   'confidence': 0.7203844809327085,\n",
       "   'extractor': 'CRFEntityExtractor'}],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.9700797200202942},\n",
       "  {'name': 'greet', 'confidence': 0.029920265078544617}],\n",
       " 'text': 'kel h Toulouse'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h Toulouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is one major limitation : the example above does not work anymore when the city name is not capitalized (toulouse instead of Toulouse). In fact, EntityExtractors traditionnally use word shapes (e.g. patterns of lowercase or uppercase letters) as predictors. In our training data, nearly all city names are capitalized. One way to correct this mistake would be to multiply examples with various capitalizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.9700797200202942},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.9700797200202942},\n",
       "  {'name': 'greet', 'confidence': 0.029920265078544617}],\n",
       " 'text': 'kel h toulouse'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h toulouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
